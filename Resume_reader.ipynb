{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d458c8c2-e5f7-4ce2-9b65-3da29b468697",
   "metadata": {},
   "source": [
    "# Below is the implementation of a resume reader, developed from scratch without relying on any pre-trained models. This approach focuses on custom extraction techniques to process and analyze resumes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fdbdff24-315d-4077-aa92-760294f88d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdf2image\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# # to unzip nltk stopwords:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# Preprocessing: load stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "934427d6-7e0d-491c-8fb9-611cb6c01b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [w for w in words if w not in stop_words]  # Remove stopwords\n",
    "    return words\n",
    "\n",
    "# # Function to check if a line is likely to be a heading (based on structure)\n",
    "# def is_heading(line, next_line):\n",
    "#     # Consider the line a heading if it's shorter (less than 5 words)\n",
    "#     if len(line.split()) <= 5:\n",
    "#         # Additionally, headings often have no punctuation and are followed by content\n",
    "#         if len(next_line.split()) > 5:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# Function to extract text from text-based PDF using pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text += text + \"\\n\"\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fff565-9390-4867-9f3a-9a31377b6fba",
   "metadata": {},
   "source": [
    "Please note that in the heading patterns, the notations are not general but in fact oriented to the resume used in the notebook. However, one should note that these headings are the ones generally used by people to construct their resume and these headings give a broad picture of different types of patterns that can be found.\n",
    "\n",
    "One should always have the domain knowledge before using the code since it might help with much more clarity with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7b6e75ba-1278-4ba1-a7ba-cee2f53cfc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract headings and descriptions\n",
    "import re\n",
    "\n",
    "def extract_headings_and_descriptions(text):\n",
    "    # Updated regex patterns for headings (no need for a colon after headings)\n",
    "    heading_patterns = [\n",
    "        r\"\\beducation\\b\",  \n",
    "        r\"\\bexperience\\b\",  \n",
    "        r\"\\bprojects\\b\",  \n",
    "        r\"\\btechnical\\s*skills\\b\",  \n",
    "        r\"\\bpositions\\s*of\\s*responsibility\\b\",  \n",
    "        r\"\\bcertifications\\b\",  \n",
    "        r\"\\bhobbies\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Join the patterns into a single regex with case-insensitive matching\n",
    "    heading_regex = re.compile(r\"(?P<heading>(\" + \"|\".join(heading_patterns) + r\"))\\s*(?:$|\\n)\", re.IGNORECASE)\n",
    "    \n",
    "    # Split text based on headings\n",
    "    sections = heading_regex.split(text)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for i in range(0, len(sections)):\n",
    "        # If the current section matches a heading (non-None and non-empty)\n",
    "        if sections[i] and heading_regex.match(sections[i].strip()):\n",
    "            heading = sections[i].strip()\n",
    "            # Get the next section as content, ensuring it's not None\n",
    "            if i + 1 < len(sections) and sections[i + 1]:\n",
    "                content = sections[i + 1].strip()\n",
    "                processed_content = preprocess_text(content)\n",
    "                result[heading] = processed_content\n",
    "    return result\n",
    "\n",
    "# # Function to dynamically extract headings and descriptions based on text structure\n",
    "# def extract_dynamic_headings(text_lines):\n",
    "#     result = {}\n",
    "#     current_heading = None\n",
    "#     current_content = []\n",
    "\n",
    "#     for i in range(len(text_lines) - 1):\n",
    "#         line = text_lines[i].strip()\n",
    "#         next_line = text_lines[i + 1].strip()  # Look ahead to the next line\n",
    "\n",
    "#         # Check if the current line is a heading\n",
    "#         if is_heading(line, next_line):\n",
    "#             # If we have accumulated content, store it under the previous heading\n",
    "#             if current_heading:\n",
    "#                 result[current_heading] = ' '.join(current_content).strip()\n",
    "#                 current_content = []  # Reset content buffer\n",
    "            \n",
    "#             current_heading = line  # Set new heading\n",
    "#         else:\n",
    "#             if current_heading:  # Only accumulate content after a heading\n",
    "#                 current_content.append(line)\n",
    "\n",
    "#     # Add the last section\n",
    "#     if current_heading:\n",
    "#         result[current_heading] = ' '.join(current_content).strip()\n",
    "\n",
    "#     return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "51333447-5131-4f5c-affc-7b005d12a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to handle both text-based and image-based PDFs\n",
    "def process_pdf(pdf_path):\n",
    "    # Check if the file is valid\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"File {pdf_path} not found!\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Processing file: {pdf_path}\")  \n",
    "    \n",
    "    # Try extracting text with pdfplumber (for text-based PDFs)\n",
    "    try:\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "        print(f\"Extracted Text: {extracted_text}\") \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "\n",
    "    # Extract headings and descriptions dynamically\n",
    "    heading_to_content = extract_headings_and_descriptions(extracted_text)\n",
    "\n",
    "\n",
    "    # Display the extracted headings and content\n",
    "    if heading_to_content:\n",
    "        for heading, content in heading_to_content.items():\n",
    "            print(f\"Heading: {heading}\")\n",
    "            print(f\"Content: {', '.join(content)}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"No headings and content extracted.\")    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8d2c4e67-7334-4f81-af72-f8c93839e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:/Users/Agam/OneDrive/Desktop/Briefcase/MY PROFILE/Agambir_Singh_Duggal_Resume_nan.pdf\n",
      "Extracted Text: AGAMBIR SINGH DUGGAL\n",
      "Mobile: +91 78887 25690 | Mail: aduggal_be22@thapar.edu |\n",
      "LinkedIn: www.linkedin.com/in/agambir-singh-duggal-073ba02aa\n",
      "EDUCATION\n",
      "Bachelor of Engineering in Computer Engineering\n",
      "Sep 2022 - Present\n",
      ". Thapar Institute of Engineering and Technology, Patiala\n",
      "Secured 8.99 absolute CGPA till 4th semester.\n",
      "Senior Secondary Education 2020 - 2022\n",
      "Sanawar Institute For Children, Bathinda\n",
      "Secured 95% marks in 12th standard.\n",
      "Secondary Education Pass out 2020\n",
      "Silver Oaks School, Bathinda\n",
      "Secured 94.4% marks in 10th standard.\n",
      "TECHNICAL EXPERIENCE\n",
      "Competitive Programming (2023- Present)\n",
      "Regular participant in coding platforms like Leetcode and GeeksforGeeks where I have solved\n",
      "100+ problems. Consistently solved algorithmic challenges based on sorting, binary search,\n",
      "greedy algorithms, dynamic programming algorithms and graph with a focus on efficiency and\n",
      "optimization.\n",
      "End to End Design and Implementation of Healthcare Website (July 2023)\n",
      "In this project we designed a healthcare website where one can donate and obtain medicines as\n",
      "per their requirements using HTML, CSS, JavaScript, Express.js, Node.js and MySQL.\n",
      "GitHub link of the project: https://github.com/agamduggal/HelpRx.git\n",
      "Supervision : Under Dr. Rajesh Gupta, Bangalore Computer Education, Bathinda\n",
      "Mammogram Classification Project (July 2024)\n",
      "Objective: Developed a machine learning model to classify mammogram images for breast\n",
      "cancer detection.\n",
      "Technologies Used: Python, TensorFlow, Keras.\n",
      "Techniques: Image preprocessing, convolutional neural networks (CNNs), data augmentation.\n",
      "Expected Outcomes: Accurate classification of mammograms with performance metrics such\n",
      "as precision, recall, and F1-score.\n",
      "TECHNICAL SKILLS\n",
      "Programming Languages: C/C++(proficient), Python, future proficiency in R\n",
      "Data Visualization: MatPlotLib, Seaborn\n",
      "Software Libraries: TensorFlow, Keras\n",
      "Deep Learning: MLPs, CNNs, RNNs\n",
      "Machine Learning Models: Statistical Analysis of SVMs, Reinforcement learning, Decision trees\n",
      "Data Science: Data Visualization, Regression Analysis, Clustering, Feature Engineering\n",
      "Big Data Technologies: Apache Spark\n",
      "Web Development: Frontend: HTML, CSS, JavaScript | Backend: Node.js, Express.js |\n",
      "Database: MySQL, Oracle database\n",
      "POSITIONS OF RESPONSIBILITY\n",
      "Actively contributed to the success of TEDx TIET by serving as a Marketing Manager\n",
      "Content and Event Manager of Thapar Mathematical Society\n",
      "Event and Discipline Manager of URJA 2023, organizing the inter technology-university sports\n",
      "fest of our college\n",
      "Actively engaged in 4 School MUNs where I was awarded High Achievers (2nd position) once\n",
      "and Verbal Appreciation (4th position) once.\n",
      "HOBBIES\n",
      "Avid reader | Passionate cricket player | Physics Enthusiast | Culinary enthusiast | Travel aficionado\n",
      "\n",
      "Heading: EDUCATION\n",
      "Content: bachelor, engineering, computer, engineering, sep, 2022, present, thapar, institute, engineering, technology, patiala, secured, 8, 99, absolute, cgpa, till, 4th, semester, senior, secondary, education, 2020, 2022, sanawar, institute, children, bathinda, secured, 95, marks, 12th, standard, secondary, education, pass, 2020, silver, oaks, school, bathinda, secured, 94, 4, marks, 10th, standard, technical\n",
      "----------------------------------------\n",
      "Heading: EXPERIENCE\n",
      "Content: competitive, programming, 2023, present, regular, participant, coding, platforms, like, leetcode, geeksforgeeks, solved, 100, problems, consistently, solved, algorithmic, challenges, based, sorting, binary, search, greedy, algorithms, dynamic, programming, algorithms, graph, focus, efficiency, optimization, end, end, design, implementation, healthcare, website, july, 2023, project, designed, healthcare, website, one, donate, obtain, medicines, per, requirements, using, html, css, javascript, express, js, node, js, mysql, github, link, project, https, github, com, agamduggal, helprx, git, supervision, dr, rajesh, gupta, bangalore, computer, education, bathinda, mammogram, classification, project, july, 2024, objective, developed, machine, learning, model, classify, mammogram, images, breast, cancer, detection, technologies, used, python, tensorflow, keras, techniques, image, preprocessing, convolutional, neural, networks, cnns, data, augmentation, expected, outcomes, accurate, classification, mammograms, performance, metrics, precision, recall, f1, score\n",
      "----------------------------------------\n",
      "Heading: TECHNICAL SKILLS\n",
      "Content: programming, languages, c, c, proficient, python, future, proficiency, r, data, visualization, matplotlib, seaborn, software, libraries, tensorflow, keras, deep, learning, mlps, cnns, rnns, machine, learning, models, statistical, analysis, svms, reinforcement, learning, decision, trees, data, science, data, visualization, regression, analysis, clustering, feature, engineering, big, data, technologies, apache, spark, web, development, frontend, html, css, javascript, backend, node, js, express, js, database, mysql, oracle, database\n",
      "----------------------------------------\n",
      "Heading: POSITIONS OF RESPONSIBILITY\n",
      "Content: actively, contributed, success, tedx, tiet, serving, marketing, manager, content, event, manager, thapar, mathematical, society, event, discipline, manager, urja, 2023, organizing, inter, technology, university, sports, fest, college, actively, engaged, 4, school, muns, awarded, high, achievers, 2nd, position, verbal, appreciation, 4th, position\n",
      "----------------------------------------\n",
      "Heading: HOBBIES\n",
      "Content: avid, reader, passionate, cricket, player, physics, enthusiast, culinary, enthusiast, travel, aficionado\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"C:/Users/Agam/OneDrive/Desktop/Briefcase/MY PROFILE/Agambir_Singh_Duggal_Resume_nan.pdf\"  # path to your PDF\n",
    "    process_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911eb13-6fc2-4761-8fbb-2148d7b5d4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
